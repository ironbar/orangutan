# Modeling

## Select modeling technique
<!---Document the actual modeling technique that is to be used. If multiple
techniques are applied, perform this task separately for each technique.
Many modeling techniques make specific assumptions about the data—for example,
that all attributes have uniform distributions, no missing values allowed,
class attribute must be symbolic, etc. Record any such assumptions made. --->

### State of the art

#### [Andrej Karpathy Deep Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/)

_May 31, 2016_

>  It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay I’m 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is end-to-end: there’s an explicit policy and a principled approach that directly optimizes the expected reward.

He explains how Policy Gradients works and illustrates it with an example of playing Pong.

#### [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/)

Publication by OpenAI that explains PPO. This method is used in the examples of AnimalAI challenge. It seems that Policy Gradients are difficult to train and one way to make it easier is to make changes in the weights that do not modify the distribution too much.

It should be implemented in [github](https://github.com/openai/baselines)

#### [Policy Gradient methods and Proximal Policy Optimization (Video)](https://www.youtube.com/watch?v=5P7I-xPq8u8)

Nice video that explains PPO. The plots show that PPO is the best PG method. At the begginning it says that DQN needs less samples than PG, however it also says that PPO is more data efficient.

If I look at the plots I can see that DQN is faster than A2C but PPO is faster than A2C so hopefully PPO and DQN will be of comparable efficiency.

<p align="center">
  <img src="https://flyyufelix.github.io/img/performance_chart.png">
</p>

<p align="center">
  <img src="media/ppo_comparison.png">
</p>


#### [Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras](https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html)

> Policy Gradients is generally believed to be able to apply to a wider range of problems. For instance, on occasions when the Q function (i.e. reward function) is too complex to be learned, DQN will fail miserably. On the other hand, Policy Gradients is still capable of learning a good policy since it directly operates in the policy space. Furthermore, Policy Gradients usually show faster convergence rate than DQN, but has a tendency to converge to a local optimal. Since Policy Gradients model probabilities of actions, it is capable of learning stochastic policies, while DQN can’t. Also, Policy Gradients can be easily applied to model continuous action space since the policy network is designed to model probability distribution, on the other hand, DQN has to go through an expensive action discretization process which is undesirable.
You may wonder if there are so many benefits of using Policy Gradients, why don’t we just use Policy Gradients all the time and forget about Q Learning? It turns out that one of the biggest drawbacks of Policy Gradients is the high variance in estimating the gradient of E[R_t]E[R​t]. Essentially, each time we perform a gradient update, we are using an estimation of gradient generated by a series of data points <ss,aa,rr,s^\primes′> accumulated through a single episode of game play. This is known as Monte Carlo method. Hence the estimation can be very noisy, and bad gradient estimate could adversely impact the stability of the learning algorithm. In contrast, when DQN does work, it usually shows a better sample efficiency and more stable performance.

#### [Proximal Policy Optimization (PPO) with Sonic the Hedgehog](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

> Some weeks ago, OpenAI made a breakthrough in Deep Reinforcement Learning. They beat some of the best Dota2 players of the world with OpenAI five, a team of 5 agents. Unfortunately, they lost during the second experience.
This breakthrough was made possible thanks to a strong hardware architecture and by using the state of the art’s algorithm: PPO aka Proximal Policy Optimization.

More evidence in favour of PPO.


## Generate test design
<!---Describe the intended plan for training, testing, and evaluating the models.
A primary component of the plan is determining how to divide the available dataset
into training, test, and validation datasets.

Doing a plot of score vs train size could be helpful to decide the validation strategy

Depending on the size of the data we have to decide how we are going to use submissions.
The less the submissions the most confidence we can have on the score. However sometimes
the data distribution is very different, or the size of the data is small and we have
to make a lot of submissions. Sometimes is not easy to have a good correlation between
validation score and LB score
--->

## Iteration 1. Iteration_title
<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.
--->

### Goal

### Development

### Results
