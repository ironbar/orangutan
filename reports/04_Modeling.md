# Modeling

## Select modeling technique
<!---Document the actual modeling technique that is to be used. If multiple
techniques are applied, perform this task separately for each technique.
Many modeling techniques make specific assumptions about the data—for example,
that all attributes have uniform distributions, no missing values allowed,
class attribute must be symbolic, etc. Record any such assumptions made. --->

### State of the art

#### [Andrej Karpathy Deep Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/)

_May 31, 2016_

>  It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay I’m 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is end-to-end: there’s an explicit policy and a principled approach that directly optimizes the expected reward.

He explains how Policy Gradients works and illustrates it with an example of playing Pong.

#### [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/)

Publication by OpenAI that explains PPO. This method is used in the examples of AnimalAI challenge. It seems that Policy Gradients are difficult to train and one way to make it easier is to make changes in the weights that do not modify the distribution too much.

It should be implemented in [github](https://github.com/openai/baselines)

#### [Policy Gradient methods and Proximal Policy Optimization (Video)](https://www.youtube.com/watch?v=5P7I-xPq8u8)

Nice video that explains PPO. The plots show that PPO is the best PG method. At the begginning it says that DQN needs less samples than PG, however it also says that PPO is more data efficient.

If I look at the plots I can see that DQN is faster than A2C but PPO is faster than A2C so hopefully PPO and DQN will be of comparable efficiency.

<p align="center">
  <img src="https://flyyufelix.github.io/img/performance_chart.png">
</p>

<p align="center">
  <img src="media/ppo_comparison.png">
</p>


#### [Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras](https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html)

> Policy Gradients is generally believed to be able to apply to a wider range of problems. For instance, on occasions when the Q function (i.e. reward function) is too complex to be learned, DQN will fail miserably. On the other hand, Policy Gradients is still capable of learning a good policy since it directly operates in the policy space. Furthermore, Policy Gradients usually show faster convergence rate than DQN, but has a tendency to converge to a local optimal. Since Policy Gradients model probabilities of actions, it is capable of learning stochastic policies, while DQN can’t. Also, Policy Gradients can be easily applied to model continuous action space since the policy network is designed to model probability distribution, on the other hand, DQN has to go through an expensive action discretization process which is undesirable.
You may wonder if there are so many benefits of using Policy Gradients, why don’t we just use Policy Gradients all the time and forget about Q Learning? It turns out that one of the biggest drawbacks of Policy Gradients is the high variance in estimating the gradient of E[R_t]E[R​t]. Essentially, each time we perform a gradient update, we are using an estimation of gradient generated by a series of data points <ss,aa,rr,s^\primes′> accumulated through a single episode of game play. This is known as Monte Carlo method. Hence the estimation can be very noisy, and bad gradient estimate could adversely impact the stability of the learning algorithm. In contrast, when DQN does work, it usually shows a better sample efficiency and more stable performance.

#### [Proximal Policy Optimization (PPO) with Sonic the Hedgehog](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

> Some weeks ago, OpenAI made a breakthrough in Deep Reinforcement Learning. They beat some of the best Dota2 players of the world with OpenAI five, a team of 5 agents. Unfortunately, they lost during the second experience.
This breakthrough was made possible thanks to a strong hardware architecture and by using the state of the art’s algorithm: PPO aka Proximal Policy Optimization.

More evidence in favour of PPO.


## Generate test design
<!---Describe the intended plan for training, testing, and evaluating the models.
A primary component of the plan is determining how to divide the available dataset
into training, test, and validation datasets.

Doing a plot of score vs train size could be helpful to decide the validation strategy

Depending on the size of the data we have to decide how we are going to use submissions.
The less the submissions the most confidence we can have on the score. However sometimes
the data distribution is very different, or the size of the data is small and we have
to make a lot of submissions. Sometimes is not easy to have a good correlation between
validation score and LB score
--->

**TODO:**

## Iteration 1. Imitation learning
<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.
--->
On the first iteration I want to train an agent by imitating human play. This will allow to create a
baseline that will allow to compare the efficiency of supervised training versus reinforcement learning.

Hopefully this agent will be able to solve many of the challenges that do not require too much intelligence
and that will also give us hints about the difficulty of the tests.

### Goal

* Learn how to train an agent, how to interact with the arena
* Train a baseline agent using imitation learning
* Learn how to make a submission
* Visualize how the agent plays

### Development

#### First submission

I have followed the [instructions](https://github.com/beyretb/AnimalAI-Olympics/blob/master/documentation/submission.md) to make a first submission using the provided agent. The submission involves
creating a docker image with all the dependencies and copying the agent inside it. There is a python script for testing the docker that I believe is very similar to the one used on the challenge.

There is python package called evalai that can be used to upload the docker image to the challenge. The upload speed is about 5 MB/s. It's possible to use that package to get the results of the submission.

I should probably adapt the docker file so I can parametrize the agent script and the data needed.

```bash
cd examples/submission/
docker build --tag=submission .
docker run -v "$PWD"/test_submission:/aaio/test submission python /aaio/test/testDocker.py
evalai push submission:latest --phase animalai-main-396
evalai submission 29473
evalai submission 29473 result
```

#### Recording games

I have created a script that allows to play games using the keyboard and save the information of
the game to disk. This allows to do later supervised learning. The script is located at "scripts/record_games/record_games.py"
and I have prepared a simple instruction on make to record games.

#### Training

I have trained a simple model that uses as input the frame, speed and previous action to predict the next action.
The number of parameters is very small ~ 6k and trains very fast.

I have realized that the games have horizontal simetry so I can duplicate the games by doing an horizontal flip to the images
and inverting the rotations.

#### Visualizing agent play

I also created a quick visualization of the agent play. It needs to be improved in the next iteration.

### Results

I will be saving the results of all the models on a [google sheet](https://docs.google.com/spreadsheets/d/15FEKXNcCCVq_YiGFdcpcruGhfKx2oJwzvRT94l4KOUY/edit#gid=0).

I have trained two models:

1. Just with games with a single static food. At the start of the games I rotated always to the same side until I saw food,
then I moved towards the food. The small food is harder to see so I played more games with small food.
2. With lot of games of different categories. I rotated to both sides because I was using mirroring of the games.

The results were surprisingly similar even when the second model was trained with much more data. I get a score of 20.67 and 19 respectively.

My guess is that the "algorithm" that I used for playing was not good for the other categories.
I think that rotating only has sense if the field is empty, otherwise exploration is a better option. If there
are obstacles is better to go to see what's behind them instead of doing a 360 rotation to finally start exploring.
Moreover moving forward at the start of the level may allow to escape from a bouncing bad goal.

### Next steps

* Try playing with a better "algorithm" that explores instead of rotating when there are obstacles
* Prepare a battery of tests that measure how well an agent plays internally
* Create a better visualization algorithm for the agent for better diagnose
* Improve the model architecture and check if better metrics are available
* Create a script for visualizing saved games


<!---
## Iteration n. Iteration_title

### Goal

### Development

### Results
--->