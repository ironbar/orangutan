# Modeling

## Select modeling technique
<!---Document the actual modeling technique that is to be used. If multiple
techniques are applied, perform this task separately for each technique.
Many modeling techniques make specific assumptions about the data—for example,
that all attributes have uniform distributions, no missing values allowed,
class attribute must be symbolic, etc. Record any such assumptions made. --->

### State of the art

#### [Andrej Karpathy Deep Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/)

_May 31, 2016_

>  It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay I’m 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is end-to-end: there’s an explicit policy and a principled approach that directly optimizes the expected reward.

He explains how Policy Gradients works and illustrates it with an example of playing Pong.

#### [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/)

Publication by OpenAI that explains PPO. This method is used in the examples of AnimalAI challenge. It seems that Policy Gradients are difficult to train and one way to make it easier is to make changes in the weights that do not modify the distribution too much.

It should be implemented in [github](https://github.com/openai/baselines)

#### [Policy Gradient methods and Proximal Policy Optimization (Video)](https://www.youtube.com/watch?v=5P7I-xPq8u8)

Nice video that explains PPO. The plots show that PPO is the best PG method. At the begginning it says that DQN needs less samples than PG, however it also says that PPO is more data efficient.

If I look at the plots I can see that DQN is faster than A2C but PPO is faster than A2C so hopefully PPO and DQN will be of comparable efficiency.

<p align="center">
  <img src="https://flyyufelix.github.io/img/performance_chart.png">
</p>

<p align="center">
  <img src="media/ppo_comparison.png">
</p>


#### [Deep Q Network vs Policy Gradients - An Experiment on VizDoom with Keras](https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html)

> Policy Gradients is generally believed to be able to apply to a wider range of problems. For instance, on occasions when the Q function (i.e. reward function) is too complex to be learned, DQN will fail miserably. On the other hand, Policy Gradients is still capable of learning a good policy since it directly operates in the policy space. Furthermore, Policy Gradients usually show faster convergence rate than DQN, but has a tendency to converge to a local optimal. Since Policy Gradients model probabilities of actions, it is capable of learning stochastic policies, while DQN can’t. Also, Policy Gradients can be easily applied to model continuous action space since the policy network is designed to model probability distribution, on the other hand, DQN has to go through an expensive action discretization process which is undesirable.
You may wonder if there are so many benefits of using Policy Gradients, why don’t we just use Policy Gradients all the time and forget about Q Learning? It turns out that one of the biggest drawbacks of Policy Gradients is the high variance in estimating the gradient of E[R_t]E[R​t]. Essentially, each time we perform a gradient update, we are using an estimation of gradient generated by a series of data points <ss,aa,rr,s^\primes′> accumulated through a single episode of game play. This is known as Monte Carlo method. Hence the estimation can be very noisy, and bad gradient estimate could adversely impact the stability of the learning algorithm. In contrast, when DQN does work, it usually shows a better sample efficiency and more stable performance.

#### [Proximal Policy Optimization (PPO) with Sonic the Hedgehog](https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e)

> Some weeks ago, OpenAI made a breakthrough in Deep Reinforcement Learning. They beat some of the best Dota2 players of the world with OpenAI five, a team of 5 agents. Unfortunately, they lost during the second experience.
This breakthrough was made possible thanks to a strong hardware architecture and by using the state of the art’s algorithm: PPO aka Proximal Policy Optimization.

More evidence in favour of PPO.


## Generate test design
<!---Describe the intended plan for training, testing, and evaluating the models.
A primary component of the plan is determining how to divide the available dataset
into training, test, and validation datasets.

Doing a plot of score vs train size could be helpful to decide the validation strategy

Depending on the size of the data we have to decide how we are going to use submissions.
The less the submissions the most confidence we can have on the score. However sometimes
the data distribution is very different, or the size of the data is small and we have
to make a lot of submissions. Sometimes is not easy to have a good correlation between
validation score and LB score
--->

**TODO:**

## Iteration 1. Imitation learning
<!---
The work is done using short iterations. Each iteration needs to have a very
clear goal. This allows to gain greater knowledge of the problem on each iteration.
--->
On the first iteration I want to train an agent by imitating human play. This will allow to create a
baseline that will allow to compare the efficiency of supervised training versus reinforcement learning.

Hopefully this agent will be able to solve many of the challenges that do not require too much intelligence
and that will also give us hints about the difficulty of the tests.

### Goal

* Learn how to train an agent, how to interact with the arena
* Train a baseline agent using imitation learning
* Learn how to make a submission
* Visualize how the agent plays

### Development

#### First submission

I have followed the [instructions](https://github.com/beyretb/AnimalAI-Olympics/blob/master/documentation/submission.md) to make a first submission using the provided agent. The submission involves
creating a docker image with all the dependencies and copying the agent inside it. There is a python script for testing the docker that I believe is very similar to the one used on the challenge.

There is python package called evalai that can be used to upload the docker image to the challenge. The upload speed is about 5 MB/s. It's possible to use that package to get the results of the submission.

I should probably adapt the docker file so I can parametrize the agent script and the data needed.

```bash
cd examples/submission/
docker build --tag=submission .
docker run -v "$PWD"/test_submission:/aaio/test submission python /aaio/test/testDocker.py
evalai push submission:latest --phase animalai-main-396
evalai submission 29473
evalai submission 29473 result
```

#### Recording games

I have created a script that allows to play games using the keyboard and save the information of
the game to disk. This allows to do later supervised learning. The script is located at "scripts/record_games/record_games.py"
and I have prepared a simple instruction on make to record games.

#### Training

I have trained a simple model that uses as input the frame, speed and previous action to predict the next action.
The number of parameters is very small ~ 6k and trains very fast.

I have realized that the games have horizontal simetry so I can duplicate the games by doing an horizontal flip to the images
and inverting the rotations.

#### Visualizing agent play

I also created a quick visualization of the agent play. It needs to be improved in the next iteration.

### Results

I will be saving the results of all the models on a [google sheet](https://docs.google.com/spreadsheets/d/15FEKXNcCCVq_YiGFdcpcruGhfKx2oJwzvRT94l4KOUY/edit#gid=0).

I have trained two models:

1. Just with games with a single static food. At the start of the games I rotated always to the same side until I saw food,
then I moved towards the food. The small food is harder to see so I played more games with small food.
2. With lot of games of different categories. I rotated to both sides because I was using mirroring of the games.

The results were surprisingly similar even when the second model was trained with much more data. I get a score of 20.67 and 19 respectively.

My guess is that the "algorithm" that I used for playing was not good for the other categories.
I think that rotating only has sense if the field is empty, otherwise exploration is a better option. If there
are obstacles is better to go to see what's behind them instead of doing a 360 rotation to finally start exploring.
Moreover moving forward at the start of the level may allow to escape from a bouncing bad goal.

### Next steps

* Try playing with a better "algorithm" that explores instead of rotating when there are obstacles
* Prepare a battery of tests that measure how well an agent plays internally
* Create a better visualization algorithm for the agent for better diagnose
* Improve the model architecture and check if better metrics are available
* Create a script for visualizing saved games

## Iteration 2. Improving Imitation Learning results

### Goal

On this iteration I want to lay the foundations for future work.

* Create an script for evaluating the agents
* Create an script for visualizing agents play
* Think of ways of improving the current score by analyzing the already submitted agents

### Development

#### Studying submission test code

The submission test outputs some scores, I want to study the code to see if I can use it as my
evaluation for the agents.

The code is quite simple and I believe I can easily modify it so it will run as many levels as config files it founds.
The scores are reproducible if the number of episodes does not change.

I have prepared a pipeline using the makefile that allows to evaluate a docker and create a summary of all evaluations.

This script could be used to create videos of the games if the agent had opencv. I could create a very similar
script to save the videos, but I don't like the idea of duplicating code and also duplicating the computing
of evaluations. Another alternative would be to save the frames using numpy and later use opencv. I think this
later one is preferable, and I have implemented it.

#### Analyzing previous agents games

Now that I have videos of the agent playing I can analyze them and think ways of improving the model.

##### Best model yet: 003_less_games

* 1_Food. It does not always aim correctly to the food but it is always able to get it.
Sometimes it reaches the borders and rotates to escape from them.
* 2_Preferences. I don't think the preference for yellow balls is clear. Sometimes it collides with red balls.
* 3_Obstacles. Only reaches the food when is visible from the start point. It does not now how to navigate the environment. Many times it gets stuck against the borders of the arena or against walls.
* 4_Avoidance. It has not learn to avoid red zones.
* 5_Spatial Reasoning. It dones not know how to navigate, sometimes it does but not very well. Again aiming
at goals is not very good, sometimes it misses them.
* 6_Generalization. It goes towards green walls, it does not know that only green balls are appealing.
* 7_Internal memory. It seems to keep moving even when the light is off, maybe I have to create an auxiliary function for the agent to stay still when lights are off.
* all_objects_random. It goes towards green walls and wood, collides with a lot of red zones.
* moving_food. Aiming is very bad

##### Model trained with more data: 002_more_games

* 2_Preferences. I don't think the preference for yellow balls is clear
* 4_Avoidence. It does not know to navigate
* 5_Spatial Reasoning. Get's stuck in the borders or walls.

#### New agent

To improve the current agent I want to focus on the following aspects:
* Better aiming at goals. I have seen that current models are not very precise, I want to improve this
* Navigation. To improve the score the model needs to be able to navigate until it sees a clear goal.
* Avoid dead goals and dead zones.
* Improve lightsOff.

I find interesting that even when it seems a simple game and I'm able to solve it defining the strategy
is not clear. So probably reinforcement learning would be the right choice to find the optimal strategy.

To achieve those goals I have to carefully design scenarios that help the agent to learn those concepts.
For example I have the hypothesis that if I start with the dead zones slightly elevated there will be collisions
with the good goals and thus the goals will never be on top of dead zones (Althought it will be interesting to learn
to ignore those goals.)

Another improvement could be to add a HUD overlay over the game that will help to aim correctly at the goal. Also a button to stop the game and save could help to give more weight to certain aspects of the game.

I will solve those problems iteratively, checking that solving one of them does not hurt the others.

##### Better aiming at goals

By using the hud I have played around 57 games and the model seems to be much accurate than previously.

### Results

I have not been able to succesfully train a better agent using supervised learning(or imitation learning).
When I played some levels of one category the score on some other category get worse. Moreover the agent
had some degenerations like going to the border of the arena and staying there.

I think that the solution has to come from Reinforcement learning. That way the model could escape those
degenerations by itself.

## Iteration 3. Reinforcement learning

### Goal

The goal of the iteration is to take the first steps with reinforcement learning.

On previous iterations we have seen
that it's possible to get a good score by showing a few examples of games played on simple scenarios. It should
be possible to do the same with RL. Moreover by playing on more complex scenarios abilities such as navigation and dead
avoiding should be developed.

### Development

#### Sample training script

On a first step I want to understand and use the sample training script provided on animalai.

### Results

<!---
## Iteration n. Iteration_title

### Goal

### Development

### Results
--->